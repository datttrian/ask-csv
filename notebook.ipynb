{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.2.1-py3-none-any.whl (973 kB)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting numpy<2,>=1\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.2 MB)\n",
      "Collecting pydantic<3,>=1\n",
      "  Using cached pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0\n",
      "  Using cached langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
      "Collecting PyYAML>=5.3\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (677 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.1 MB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.2 MB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Using cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.0\n",
      "  Using cached langchain_core-0.2.3-py3-none-any.whl (310 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Using cached langsmith-0.1.69-py3-none-any.whl (124 kB)\n",
      "Collecting requests<3,>=2\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Using cached dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (297 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (126 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (238 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Using cached orjson-3.10.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (149 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/vscode/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.12.1)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.18.4\n",
      "  Using cached pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.8 MB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (138 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (651 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: urllib3, tenacity, PyYAML, pydantic-core, packaging, orjson, numpy, mypy-extensions, multidict, jsonpointer, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic, marshmallow, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 attrs-23.2.0 certifi-2024.6.2 charset-normalizer-3.3.2 dataclasses-json-0.6.6 frozenlist-1.4.1 greenlet-3.0.3 idna-3.7 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-community-0.2.1 langchain-core-0.2.3 langchain-text-splitters-0.2.0 langsmith-0.1.69 marshmallow-3.21.2 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.3 packaging-23.2 pydantic-2.7.3 pydantic-core-2.18.4 requests-2.32.3 tenacity-8.3.0 typing-inspect-0.9.0 urllib3-2.2.1 yarl-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/langchain/_api/module_import.py:92: LangChainDeprecationWarning: Importing DirectoryLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.document_loaders import DirectoryLoader\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.document_loaders import DirectoryLoader\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here https://python.langchain.com/v0.2/docs/versions/v0_2/ \n",
      "  warn_deprecated(\n",
      "/home/vscode/.local/lib/python3.10/site-packages/langchain/_api/module_import.py:92: LangChainDeprecationWarning: Importing Chroma from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import Chroma\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import Chroma\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here https://python.langchain.com/v0.2/docs/versions/v0_2/ \n",
      "  warn_deprecated(\n",
      "Split 1 documents into 298 chunks.\n",
      "18 A Permanent Revolution 19 And They Lived Happily Ever After 20 The End of Homo Sapiens\n",
      "\n",
      "Afterword: The Animal that Became a God\n",
      "\n",
      "Notes Acknowledgements Image credits\n",
      "\n",
      "Timeline of History\n",
      "\n",
      "Years\n",
      "\n",
      "Before\n",
      "\n",
      "the\n",
      "\n",
      "Present\n",
      "\n",
      "13.5\n",
      "\n",
      "Matter and energy appear. Beginning of physics. Atoms and molecules\n",
      "{'source': 'data/books/index.pdf', 'start_index': 2057}\n",
      "/home/vscode/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n",
      "Saved 298 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "!python project.py index.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Please ask a question related to the PDF file!\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    query_text = \"Why humans run the world\"\n",
    "\n",
    "    # Prepare the DB.\n",
    "    embedding_function = OpenAIEmbeddings()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    print(results)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Please ask a question related to the PDF file!\")\n",
    "        return\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohttp==3.9.5\n",
      "aiosignal==1.3.1\n",
      "annotated-types==0.7.0\n",
      "antlr4-python3-runtime==4.9.3\n",
      "anyio==4.4.0\n",
      "asgiref==3.8.1\n",
      "asttokens==2.4.1\n",
      "async-timeout==4.0.3\n",
      "attrs==23.2.0\n",
      "backoff==2.2.1\n",
      "bcrypt==4.1.3\n",
      "beautifulsoup4==4.12.3\n",
      "build==1.2.1\n",
      "cachetools==5.3.3\n",
      "certifi==2024.6.2\n",
      "cffi==1.16.0\n",
      "chardet==5.2.0\n",
      "charset-normalizer==3.3.2\n",
      "chroma-hnswlib==0.7.3\n",
      "chromadb==0.5.0\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "coloredlogs==15.0.1\n",
      "comm==0.2.2\n",
      "contourpy==1.2.1\n",
      "cryptography==42.0.7\n",
      "cycler==0.12.1\n",
      "dataclasses-json==0.6.6\n",
      "debugpy==1.8.1\n",
      "decorator==5.1.1\n",
      "deepdiff==7.0.1\n",
      "Deprecated==1.2.14\n",
      "distro==1.9.0\n",
      "effdet==0.4.1\n",
      "emoji==2.12.1\n",
      "exceptiongroup==1.2.1\n",
      "executing==2.0.1\n",
      "fastapi==0.110.3\n",
      "filelock==3.14.0\n",
      "filetype==1.2.0\n",
      "flatbuffers==24.3.25\n",
      "fonttools==4.53.0\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.6.0\n",
      "gitdb==4.0.11\n",
      "GitPython==3.1.41\n",
      "google-api-core==2.19.0\n",
      "google-auth==2.29.0\n",
      "google-cloud-vision==3.7.2\n",
      "googleapis-common-protos==1.63.1\n",
      "greenlet==3.0.3\n",
      "grpcio==1.64.1\n",
      "grpcio-status==1.62.2\n",
      "h11==0.14.0\n",
      "httpcore==1.0.5\n",
      "httptools==0.6.1\n",
      "httpx==0.27.0\n",
      "huggingface-hub==0.23.2\n",
      "humanfriendly==10.0\n",
      "idna==3.7\n",
      "importlib_metadata==7.1.0\n",
      "importlib_resources==6.4.0\n",
      "iniconfig==2.0.0\n",
      "iopath==0.1.10\n",
      "ipykernel==6.29.4\n",
      "ipython==8.25.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.4\n",
      "joblib==1.4.2\n",
      "jsonpatch==1.33\n",
      "jsonpath-python==1.0.6\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.22.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter_client==8.6.2\n",
      "jupyter_core==5.7.2\n",
      "kiwisolver==1.4.5\n",
      "kubernetes==29.0.0\n",
      "langchain==0.2.1\n",
      "langchain-cli==0.0.24\n",
      "langchain-community==0.2.1\n",
      "langchain-core==0.2.3\n",
      "langchain-openai==0.1.8\n",
      "langchain-text-splitters==0.2.0\n",
      "langdetect==1.0.9\n",
      "langserve==0.2.1\n",
      "langsmith==0.1.69\n",
      "layoutparser==0.3.4\n",
      "libcst==1.4.0\n",
      "lxml==5.2.2\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.5\n",
      "marshmallow==3.21.2\n",
      "matplotlib==3.9.0\n",
      "matplotlib-inline==0.1.7\n",
      "mdurl==0.1.2\n",
      "mmh3==4.1.0\n",
      "monotonic==1.6\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "mypy-extensions==1.0.0\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.3\n",
      "nltk==3.8.1\n",
      "numpy==1.26.4\n",
      "oauthlib==3.2.2\n",
      "omegaconf==2.3.0\n",
      "onnx==1.16.1\n",
      "onnxruntime==1.18.0\n",
      "openai==1.31.0\n",
      "opencv-python==4.10.0.82\n",
      "opentelemetry-api==1.25.0\n",
      "opentelemetry-exporter-otlp-proto-common==1.25.0\n",
      "opentelemetry-exporter-otlp-proto-grpc==1.25.0\n",
      "opentelemetry-instrumentation==0.46b0\n",
      "opentelemetry-instrumentation-asgi==0.46b0\n",
      "opentelemetry-instrumentation-fastapi==0.46b0\n",
      "opentelemetry-proto==1.25.0\n",
      "opentelemetry-sdk==1.25.0\n",
      "opentelemetry-semantic-conventions==0.46b0\n",
      "opentelemetry-util-http==0.46b0\n",
      "ordered-set==4.1.0\n",
      "orjson==3.10.3\n",
      "overrides==7.7.0\n",
      "packaging==23.2\n",
      "pandas==2.2.2\n",
      "parso==0.8.4\n",
      "pdf2image==1.17.0\n",
      "pdfminer.six==20231228\n",
      "pdfplumber==0.11.0\n",
      "pexpect==4.9.0\n",
      "pikepdf==9.0.0\n",
      "pillow==10.3.0\n",
      "pillow_heif==0.16.0\n",
      "platformdirs==4.2.2\n",
      "pluggy==1.5.0\n",
      "portalocker==2.8.2\n",
      "posthog==3.5.0\n",
      "prompt_toolkit==3.0.45\n",
      "proto-plus==1.23.0\n",
      "protobuf==4.25.3\n",
      "psutil==5.9.8\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pyasn1==0.6.0\n",
      "pyasn1_modules==0.4.0\n",
      "pycocotools==2.0.7\n",
      "pycparser==2.22\n",
      "pydantic==2.7.3\n",
      "pydantic_core==2.18.4\n",
      "Pygments==2.18.0\n",
      "pyparsing==3.1.2\n",
      "pypdf==4.2.0\n",
      "pypdfium2==4.30.0\n",
      "PyPika==0.48.9\n",
      "pyproject-toml==0.0.10\n",
      "pyproject_hooks==1.1.0\n",
      "pytesseract==0.3.10\n",
      "pytest==8.2.1\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.0.1\n",
      "python-iso639==2024.4.27\n",
      "python-magic==0.4.27\n",
      "python-multipart==0.0.9\n",
      "pytz==2024.1\n",
      "PyYAML==6.0.1\n",
      "pyzmq==26.0.3\n",
      "rapidfuzz==3.9.3\n",
      "referencing==0.35.1\n",
      "regex==2024.5.15\n",
      "requests==2.32.3\n",
      "requests-oauthlib==2.0.0\n",
      "rich==13.7.1\n",
      "rpds-py==0.18.1\n",
      "rsa==4.9\n",
      "safetensors==0.4.3\n",
      "scipy==1.13.1\n",
      "shellingham==1.5.4\n",
      "six==1.16.0\n",
      "smmap==5.0.1\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.5\n",
      "SQLAlchemy==2.0.30\n",
      "sse-starlette==1.8.2\n",
      "stack-data==0.6.3\n",
      "starlette==0.37.2\n",
      "sympy==1.12.1\n",
      "tabulate==0.9.0\n",
      "tenacity==8.3.0\n",
      "tiktoken==0.7.0\n",
      "timm==1.0.3\n",
      "tokenizers==0.19.1\n",
      "toml==0.10.2\n",
      "tomli==2.0.1\n",
      "tomlkit==0.12.5\n",
      "torch==2.3.0\n",
      "torchvision==0.18.0\n",
      "tornado==6.4\n",
      "tqdm==4.66.4\n",
      "traitlets==5.14.3\n",
      "transformers==4.41.2\n",
      "typer==0.9.4\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.12.1\n",
      "tzdata==2024.1\n",
      "unstructured==0.14.4\n",
      "unstructured-client==0.22.0\n",
      "unstructured-inference==0.7.33\n",
      "unstructured.pytesseract==0.3.12\n",
      "urllib3==2.2.1\n",
      "uvicorn==0.23.2\n",
      "uvloop==0.19.0\n",
      "watchfiles==0.22.0\n",
      "wcwidth==0.2.13\n",
      "websocket-client==1.8.0\n",
      "websockets==12.0\n",
      "wrapt==1.16.0\n",
      "yarl==1.9.4\n",
      "zipp==3.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing (apple, apple): {'score': -1.1102230246251565e-15}\n",
      "Comparing (apple, iphone): {'score': 0.09710853291781563}\n",
      "Comparing (apple, banana): {'score': 0.09725941975023544}\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "\n",
    "evaluator = load_evaluator(\"pairwise_embedding_distance\")\n",
    "\n",
    "words = (\"apple\", \"apple\")\n",
    "x = evaluator.evaluate_string_pairs(prediction=words[0], prediction_b=words[1])\n",
    "print(f\"Comparing ({words[0]}, {words[1]}): {x}\")\n",
    "\n",
    "words = (\"apple\", \"iphone\")\n",
    "x = evaluator.evaluate_string_pairs(prediction=words[0], prediction_b=words[1])\n",
    "print(f\"Comparing ({words[0]}, {words[1]}): {x}\")\n",
    "\n",
    "words = (\"apple\", \"banana\")\n",
    "x = evaluator.evaluate_string_pairs(prediction=words[0], prediction_b=words[1])\n",
    "print(f\"Comparing ({words[0]}, {words[1]}): {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ask a question related to the PDF file!\n",
      "[]\n",
      "Human: \n",
      "You are an intelligent assistant. You have been provided with the following context extracted from a PDF document:\n",
      "\n",
      "\n",
      "\n",
      "Based on this context, please provide a simple answer to the following question:\n",
      "\n",
      "What is money?\n",
      "\n",
      "Response: Money is a medium of exchange used to facilitate transactions and represent value.\n",
      "Sources: []\n"
     ]
    }
   ],
   "source": [
    "query_text = \"What is money?\"\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(\"Please ask a question related to the PDF file!\")\n",
    "print(results)\n",
    "\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "print(prompt)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "response_text = model.predict(prompt)\n",
    "\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 220 chunks.\n",
      "16,000 Sapiens settle America. Extinction of American megafauna.\n",
      "\n",
      "13,000\n",
      "\n",
      "Extinction of Homo floresiensis. Homo sapiens the only surviving human\n",
      "\n",
      "species.\n",
      "\n",
      "12,000\n",
      "\n",
      "The Agricultural Revolution. Domestication of plants and animals.\n",
      "\n",
      "Permanent settlements.\n",
      "\n",
      "5,000\n",
      "\n",
      "First kingdoms, script and money. Polytheistic religions.\n",
      "\n",
      "4,250\n",
      "\n",
      "First empire – the Akkadian Empire of Sargon.\n",
      "{'source': 'data/books/index.pdf', 'start_index': 3085}\n",
      "Please ask a question related to the PDF file!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/books\"\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an intelligent assistant. You have been provided with the following context extracted from a PDF document:\n",
    "\n",
    "{context}\n",
    "\n",
    "Based on this context, please provide an answer to the following question:\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    pdf_file = \"index.pdf\"\n",
    "    documents = load_documents(pdf_file)\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)\n",
    "\n",
    "    query_text = \"Why humans run the world?\"\n",
    "    embedding_function = OpenAIEmbeddings()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(\"Please ask a question related to the PDF file!\")\n",
    "        return\n",
    "\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "\n",
    "\n",
    "def load_documents(file):\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=file)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    Chroma.from_documents(chunks, OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
